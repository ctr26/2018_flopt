%% \def\osa{} %% set to true

\def\nature{} %% set to true
%% \def\bioarxiv{} %% set to true

\ifdefined\osa
    \documentclass{osa-article}
    \usepackage{subcaption}
    \journal{osac}
\fi

\ifdefined\nature
    \documentclass{wlscirep}
    \usepackage{subcaption}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \thispagestyle{empty}
\fi

\ifdefined\bioarxiv
    \documentclass[times, twoside]{zHenriquesLab-StyleBioRxiv}
    \leadauthor{Russell}
\fi
%% Select the journal you're submitting to
%% oe, boe, ome, osac, osajournal


% \usepackage{blindtext}
\usepackage{siunitx}
\usepackage{pgfplots}
\usepackage{tikzscale}
\usepackage{tikz}
\usepackage{graphicx}
\usetikzlibrary{positioning}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
   \hskip -\arraycolsep
   \let\@ifnextchar\new@ifnextchar
   \array{#1}}
\makeatother

\pgfplotsset{
  tick label style = {font=\sansmath\sffamily\footnotesize},
  %every axis label = {font=\sansmath\sffamily},
  %legend style = {font=\sansmath\sffamily},
  %label style = {font=\sansmath\sffamily}
}
% \usepackage{tabular}
\usepackage{booktabs}
\usepackage{pdflscape}
% \usepackage{hyperref}
\usepackage{url}
\usepackage{graphbox}
\usepackage{lineno}

\usepackage{algorithm}
\usepackage{algorithmic}



% \usepackage[utf8x]{inputenc}

 % \usepackage{url}
% Please give the surname of the lead author for the running footer

% \ifpdf{}
%     \graphicspath{{Figs/Raster/}{Figs/PDF/}{Figs/}}
% \else
%     \graphicspath{{Figs/Vector/}{Figs/}}
% \fi

% \input{glossary}
\RequirePackage[xindy,acronym,toc,symbols]{glossaries-extra}

\makeglossaries{}
\makeindex

\setabbreviationstyle[acronym]{long-short}
\glssetcategoryattribute{acronym}{dualindex}{true}
\glssetcategoryattribute{general}{dualindex}{true}
%Computer vision
\glsxtrnewsymbol[description={World coordinates, \((X, Y, Z)\)}]{X}{\ensuremath{\mathbf{X}}}
\glsxtrnewsymbol[description={Camera-centered world coordinate,\((X_c, Y_c, Z_c)\), left camera}]{X_c}{\ensuremath{\mathbf{X_c}}}
\glsxtrnewsymbol[description={Camera-centered world coordinate,\((X_c', Y_c', Z_c')\) right camera}]{X_c'}{\ensuremath{\mathbf{X_c'}}}
\glsxtrnewsymbol[description={Ray to point on image plane,\((x,y,f)\), left camera}]{p}{\ensuremath{\mathbf{p}}}
\glsxtrnewsymbol[description={Ray to point on image plane,\((x,y,f)\), right camera}]{p'}{\ensuremath{\mathbf{p'}}}
\glsxtrnewsymbol[description={Image plane coordinates,\((x,y)\), left camera}]{x}{\ensuremath{\mathbf{x}}}
\glsxtrnewsymbol[description={Image plane coordinates,\((x,y)\), right camera}]{x'}{\ensuremath{\mathbf{x'}}}
\glsxtrnewsymbol[description={Pixel coordinates,\((u,v)\), left camera}]{w}{\ensuremath{\mathbf{w}}}
\glsxtrnewsymbol[description={Pixel coordinates,\((u,v)\), right camera}]{w'}{\ensuremath{\mathbf{w'}}}
\glsxtrnewsymbol[description={Camera parameters, left camera}]{K}{\ensuremath{\mathbf{K}}}
\glsxtrnewsymbol[description={Camera parameters, right camera}]{K'}{\ensuremath{\mathbf{K'}}}

\glsxtrnewsymbol[description={Rotation matrix (orthonormal)}]{R}{\ensuremath{\mathbf{R}}}
\glsxtrnewsymbol[description={Translation vector (3 element)}]{T}{\ensuremath{\mathbf{T}}}
\glsxtrnewsymbol[description={The Fundamental matrix}]{F}{\ensuremath{\mathbf{F}}}
\glsxtrnewsymbol[description={The Essential matrix}]{Essential}{\ensuremath{\mathbf{E}}}
\glsxtrnewsymbol[description={The Homography matrix}]{H}{\ensuremath{\mathbf{H}}}
\glsxtrnewsymbol[description={\(3\times3\) matrix containing coordinates from \gls{T}}]{T_x}{\ensuremath{\mathbf{T_\times}}}
\glsxtrnewsymbol[description={Mean square error: \(\operatorname{MSE}=\frac{1}{n}\sum_{i=1}^n{(Y_i-\hat{Y_i})}^2\)}]{MSE}{\ensuremath{MSE}}


% \usepackage[backend=biber,style=nature]{biblatex}
% \addbibresource{merge.bib}

\begin{document}

\title{Frame Localisation Optical Projection Tomography}

\ifdefined\nature

  \author[1,*]{Alice Author}
  \author[2]{Bob Author}
  \author[1,2,+]{Christine Author}
  \author[2,+]{Derek Author}
  \affil[1]{Affiliation, department, city, postcode, country}
  \affil[2]{Affiliation, department, city, postcode, country}

  \affil[*]{corresponding.author@email.example}

  \affil[+]{these authors contributed equally to this work}
  \flushbottom
\fi
\ifdefined\osa
  \author{Craig Russell,\authormark{1} and Eric Rees,\authormark{2,*}}

  \address{\authormark{1}Dept. of Chemical Engineering and Biotechnology, Cambridge University, Cambridge, U.K. \\
    \authormark{2} European Bioinformatics Institute, Wellcome Genome Campus, Cambridge CB10 1SD}
  \email{\authormark{*}ctr26@ebi.ac.uk} %% email address is required
\fi
\ifdefined\bioarxiv
  \title{Frame Localisation Optical Projection Tomography}
  \shorttitle{flOPT}
  % Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
  \author[1,2 \Letter]{Craig Russell}
  \author[1]{Pedro P Vallejo Ramirez}
  \author[1]{Eric Rees}



  \affil[1]{Dept.~of Chemical Engineering and Biotechnology, Cambridge University, Cambridge, U.K.}
  \affil[2]{European Bioinformatics Institute, Wellcome Genome Campus, Cambridge CB10 1SD}

  \maketitle

  %TC:break Abstract
  %the command above serves to have a word count for the abstract
  \linenumbers{}

  %TC:break main
  %the command above serves to have a word count for the abstract

  \begin{keywords}
    OPT, reconstruction, CT, 3D-imaging,
  \end{keywords}

  \begin{corrauthor}
    %\texttt{r.henriques{@}ucl.ac.uk}
    ctr26\at ebi.ac.uk
  \end{corrauthor}

  %TC:endignore
  %the command above ignores this section for word count
\fi

\begin{abstract}
  We present a tomographic reconstruction algorithm (flOPT), which is applied to Optical Projection Tomography (OPT) images, that is robust to mechanical jitter and systematic angular and spatial drift.
  OPT relies on precise mechanical rotation and is less mechanically stable than large-scale computer tomography (CT) scanning systems, leading to reconstruction artefacts.
  The algorithm uses multiple (5+) tracked fiducial beads to recover the sample pose and the image rays are then back-projected at each orientation.
  The quality of the image reconstruction using the proposed algorithm shows an improvement when compared to the Radon transform.
  Moreover, when adding a systematic spatial and angular mechanical drift, the reconstruction shows a significant improvement over the Radon transform.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%  body  %%%%%%%%%%%%%%%%%%%%%%%%%%

% \section*{Frame localisation optical projection tomography}

% % \epigraph{\emph{Pascale Sauvage}}{--- Pablo Vallejo}
%
% % Volumetric imaging can also be achieved by rotating a sample and %reconstructing tomographically.
% % tomographically reconstructing the 3D distribution of a signal (scattering, absorption or luminosity) within the specimen.
% %Orthogonal imaging schema can be replaced with pass through imaging provide samples are sufficiently transparent.
% %Instead of scanning these samples laterally one can rotate their sample and reconstruct a full three dimensional image.
% %As tomographic technology has been shrunk to the millimeter scale, errors induced by hardware become apparent.
% Accurate %reconstructions of volumes rely on
% tomographic reconstruction relies
% heavily on precision movement and rotation.
% %This chapter addresses a key downside in traditional approaches to performing full three dimensional reconstructions tomographically.
% Here an algorithm will be presented that relies exclusively on multiple (4+) tracked fiducial beads to enable accurate reconstruction even with systematic mechanical drift.
% It will be demonstrated on ground truth simulated testcard image data with motion errors compounded onto the tomographic rotation.
% These motion errors will include systematic mechanical drift, giving a spiral path; and angular drift giving precession.

% \section*{Rotational computer tomography in microscopy}

%Well established
%Three-dimensional imaging of anatomy in thick biological samples provides valuable data for developmental biology studies.
%Tomographic techniques that generate 3D reconstructions from 2D images such as computed tomography (CT) and magnetic resonance imaging (MRI) are essential in medical applications to visualize morphology in large tissues and organs.
%CT and especially micro-CT can achieve micron-scale resolution using certain contrast agents, however the high doses of radiation used make this unsuitable for repeated experiments on a biological sample.
%Micro-MRI can also achieve resolution in the micron scale, however the cost and size of MRI instruments can be prohibitive for many applications[21].
%Furthermore, neither of these techniques can exploit the plethora of information that can be extracted through fluorescence microscopy.

Sharpe~\emph{et.~al} proposed OPT~\cite{sharpeOpticalProjectionTomography2002}
using visible light to image transparent or translucent mesoscopic samples, with micrometer resolution.
OPT addresses the scale gap between photographic techniques (for samples typically larger than \SI{10}{\milli\meter}), and light microscopy techniques (samples smaller than \SI{1}{\milli\meter}) to image biological samples in the \SIrange{1}{10}{\milli\meter} range.
% OPT is non-invasive optically but may require specialist invasive preparation for its samples.
% TODO: PV photographic
%Optical Projection Tomography was first proposed by Sharpe in 2002 [30]; it uses visible light to image and create volumetric data of transparent (naturally or artificially) mesoscopic objects (1 - 10 mm) at micron-level resolution.

OPT is based on computerised tomography techniques~\cite{kakPrinciplesComputerizedTomographic2001} in which a set of projections of a specimen are acquired as the specimen travels through a full rotation, shown in \figurename~\ref{fig:OPT_digram}.
Typically, a Radon transform is then used to transform this set of images into a 3D image stack in Cartesian coordinates (\( X,Y,Z \)).
%A cross-sectional stack of slices from the original object is reconstructed using a back-projection algorithm from the projection images.
The Radon transform relies heavily on the assumption of circular motion with constant angular steps about a vertical axis.
Prior to the Radon transform, an attempt is made to find the centre of rotation (CORs) and correct the image shift~\cite{dongAutomatedRecoveryCenter2013,arranzHelicalOpticalProjection2013,zhangZOPTOpenSource2020}; this step is both computationally expensive, error prone and incomplete with regards to all available degrees of freedom.
This work presents an improved general reconstruction algorithm that is robust to spatial and angular mechanical drifts during acquisitions, as well as to inconsistent angular steps.
The proposed algorithm triangulates points between image pairs to extract camera pose using the theoretical framework used in stereoscopic imaging. %TODO reword
% There are two imaging modalities for OPT, eOPT and tOPT.
% In eOPT, a fluorescent sample is excited using an illumination source off axis to the detection, similar to light-sheet but the entire depth of field of the detection of objective is illuminated.
% % without the excitation being shaped into a sheet.
% Scattered illumination photons are rejected at the detector using an appropriate filter.
% In tOPT, a white-light source with a diffuser and a collimator is placed along the optical axis to provide near-collimated, uniform illumination onto the sample for transmission to a detector opposite (see \figurename~\ref{fig:OPT_digram}).
% Each photosite at the detector corresponds to a ray that has passed through the sample and been attenuated by the sample.
% The eOPT and tOPT modes can work in unison to provide contextual information, with the transmission images indicating overall structure (optical density of absorption or scattering) which can be supplemented by the fluorescent signal from a label of interest.
%
% \begin{figure*}
%   \centering
%   \includegraphics{./figures/OPT_digram}
%   \caption[Principle of OPT]{Principle of OPT, a respective rotation between the sample and the detector illumination pair is iterated.
%   The volumetric image is later reconstructed from the set of detections (1/2D).}
%   \label{fig:OPT_digram}
% \end{figure*}


\section*{Stereoscopic imaging}

\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics{./figures/coordinate_system}
    \caption[Coordinate system]{Coordinate system describing a camera with an associated image plane one focal distance \(f\) away, imaging an object at point \(X\).
    }\label{fig:coordinate_system_flopt}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics{./figures/OPT_digram}
    \caption[Principle of OPT]{From an angle \(\theta \), an object \(f(X,Y)\) and its projection \(P_\theta(\nu)\) are known.}\label{fig:OPT_digram}
    %{Principle of OPT, a respective rotation between the sample and the detector illumination pair is iterated.
    % The volumetric image is later reconstructed from the set of detections (1/2D).}
  \end{subfigure}
  \caption[Coordinates and OPT]{\gls*{X_c} = \((X_c,Y_c,Z_c)\) is the camera-centered coordinate point in 3D space.
    \gls*{X} = \((X,Y,Z)\) is the world coordinate point in 3D space.
    \gls*{p} = \((x,y,f)\) is the ray vector to point of image plane.
    \gls*{x} = \((x,y)\) is the image plane coordinates.
    \gls*{w} = \((u,v)\) are the pixel coordinates (not shown) corresponding to the point \gls*{x}.
    The optical axis travels along the \(Z_c\) axis through the image plane.}\label{fig:single-view-coordinates}
\end{figure*}
\begin{figure*}
  \centering
  \includegraphics{./figures/epi-polar-geom}
  \caption[Epi-polar geometry described for two adjacent views]{
    Epi-polar geometry described for two adjacent views (or cameras of a scene).
    Coordinates as expressed in \figurename~\ref{fig:coordinate_system_flopt} with prime notation (\('\)) denoting the additional right camera view.
    Transforming from right to left camera-centered coordinates (\gls*{X_c'} to \gls*{X_c}) requires a rotation (\gls*{R}) and a translation (\gls*{T}).
  }\label{fig:epi-polar-geom}
\end{figure*}

%\subsection{Projective geometry}

%Camera imaging is governed by projective geometry
%Parallel lines project onto a camera will have a vanishing point at the horizon.

%\subsection{Camera projections}

When the features or fiducial markers in one view are uniquely identifiable, the stereoscopic imaging of scenes allows for the triangulation of individual features in three dimensional space (known as world points), see \figurename~\ref{fig:epi-polar-geom} and \figurename~\ref{fig:single-view-coordinates} for the coordinate system which describes this geometry. %(such as fluorescent beads).
Triangulation requires that each feature is detected in both images of a stereo imaging system and for these detections to be correctly associated with one another.
This is known as the correspondence problem.
Various methods exist to ensure that features are detected from image data and accurately associated between two cameras or views~\cite{szeliskiComputerVisionAlgorithms2010} and the properties of scale-independent features and their surrounding pixel environment in one image can thus be matched to a similar feature in a second image.

Coordinates in two adjacent views with a common epi-pole (the vector connecting the \(O\) and \(O'\), see \figurename~~\ref{fig:epi-polar-geom}) are related by the essential matrix (\gls*{Essential}) for uncalibrated cameras and the fundamental matrix (\gls*{F}) for calibrated cameras.
Their properties are described by:
\begin{gather}
  \gls*{p'}^T \gls*{Essential} \gls*{p} = 0 \label{eq:pEp}\\
  \gls*{Essential} = \gls*{K'}^T \gls*{F} \gls*{K} %= \gls*{T_x} \gls*{R} = \mathbf{U}\Lambda \mathbf{V}^T%\gls*{R} = U\LambdaV^T\\
\end{gather}
Where \(\gls*{K}\) is a matrix that converts image plane coordinates to camera pixel coordinates and where \gls*{p} refers to a point in the image plane.

\section*{The proposed algorithm (flOPT)}

% R1.2
\begin{algorithm}
  \begin{algorithmic}
    \caption{The proposed flOPT algorithm, where:
      \emph{localiseFiducials} generates a 2D coordinate list from an image containing punctate fiducials.
      \emph{findHomography} generates \(\mathbf{F}_n\) between a pair of 2D coordinate lists.
      \emph{chooseBestHomography} is a function that chooses the most likely homography matrix \(\mathbf{F}_n\) but minimising the least squares difference between \(\mathbf{F}_{n-1}\) and the four candidate \(\mathbf{F}_{n,\{0,1,2,3\}}\) matrices.
      \emph{decomposeHomography} factorises \(\mathbf{F}_n\) into affine rotation and translation matrices (\(\mathbf{R}_n\) and  \(\mathbf{T}_n\)).
      \emph{affineHomogenise} merges the separate affine matrices into a more manageable homogenous matrix.
      \emph{spatialFilter} finally corrects the image using standard computer tomography filters.
    }
    \STATE{\(n \leftarrow 1\)}
    \STATE{\(\mathbf{x} \leftarrow \text{localiseFiducials}(I_0)\)}
    \WHILE{\(n \leq k\)}
    \STATE{\(\mathbf{x}' \leftarrow \text{localiseBeads}(I_n)\)}
    \STATE{\(\mathbf{F}_{n,\{0,1,2,3\}} \leftarrow \text{findHomography}(\mathbf{x},\mathbf{x}')\)}
    \STATE{\(\mathbf{F}_n \leftarrow \text{chooseBestHomography}(\mathbf{F}_{n,\{0,1,2,3\}}, \mathbf{F}_{n-1} )\)}
    \STATE{\(\mathbf{R}_n, \mathbf{T}_n \leftarrow \text{decomposeHomography}(\mathbf{F}_n)\)}
    % \STATE decompose F to find R and T
    % \STATE [R|T] affineHomogenise(R_n,T_N)
    %\STATE \(\mathbf{R_n}, \mathbf{T_n} \leftarrow \text{decomposeHomography}(\mathbf{F})\)
    \STATE{\([\mathbf{R}_n |\mathbf{T}_n] \leftarrow \text{affineHomogenise}(\mathbf{R}_n, \mathbf{T}_n)\)}
    % \STATE invert [R|T] and apply to I i+1
    \STATE{\(I^\text{*}_{n} \leftarrow [\mathbf{R}_n |\mathbf{T}_n]^{-1} \cdot I_{n}\)}
    \STATE{\(I_\text{unfiltered} \leftarrow \sum_{n=0}^k I^*_{n}\)}
    \STATE{\(n \leftarrow n\)}
    \ENDWHILE{}
    \STATE{\(I_\text{final} \leftarrow \text{spatialFilter}(I_\text{unfiltered})\)}
    \RETURN{\(I_\text{final}\)}
  \end{algorithmic}
\end{algorithm}

%The rotation may also not be orthogonal to the plane of detection.
The motion of a rotating sample, as in an OPT acquisition, with a transformation matrix (\( \begin{bmatrix}[c|c] \gls*{R} & \gls*{T} \end{bmatrix}\)) in view of a fixed camera is analogous to the motion of a camera around the scene with the inverse transformation matrix. %TODO reword
% The shift of a camera around a scene separated by a transformation matrix (\( \begin{bmatrix}[c|c] \gls*{R} & \gls*{T} \end{bmatrix}\)) is analogous to transforming the sample in the fixed view of an imaging detector, as in an \gls*{OPT} acquisition.
During an ideal OPT acquisition, a marker will appear to follow an elliptical path in the \(xy\) image plane.
For the volume reconstruction procedure, there is a fitting step to recover the path of the fiducial marker, which is used to correct the sinogram before applying the inverse Radon transform.
This type of reconstruction not only ignores any mechanical jitter of the sample, but also any affine, systematic, mechanical drift (in \(X,Y,Z,\theta,\phi,\psi \)). %TODO is this true?
This can be rectified by recovering the complete non-scaling transformation for every projection.
Now, using two adjacent images of a scene (separated by some rotation and translation) world points in 3D space may be triangulated within the scene given the rotational and translational matrices of the respective camera views.

% The inverse is also possible, given a sufficient number of known fiducial points in a scene the translation and rotation matrices can be recovered.

% The recovery of a more exact description of the motion of the scene can eliminate any need for a fitting and may recover and correct for drift, as well as eliminate any mechanical jitter.
% Errors may however then be introduced from fiducial markers mechanically slipping and localisation errors.
% Fiducial markers in this sense refer to an accurately locatable marker common through different views in a sample.

Once a sufficient amount of fiducial markers are reliably tracked from the first to the second image, either one of the fundamental or essential matrices can be computed.
Using the factorisation of one of these matrices, between each adjacent view of a rotating scene, the translation and rotational matrices can be recovered.
% Here we will discuss a reconstruction using \gls*{F} but the same principle applies for \gls*{Essential} and \gls*{H}.

% Here are two ways of reconstructing using the fundamental matrix as described above.
To reconstruct the image, we compute \gls*{F} for the current image and the first image using 5 or more fiducial markers; having additional beads helps to remove ambiguity and increase confidence in \gls*{F}.
Once \gls*{F} is calculated, it is decomposed into \(\gls*{R}_n\) and \(\gls*{T}_n\) between each view \(n\) and \(n+1\).
The image at view \(n+1\) is then back projected along the virtual optical axis within a virtual volume where the sample will be reconstructed.
The size of this back projection and virtual volume is chosen to be suitably large, preventing the loss of important data.
% Then, all the prior rotation and translation matrices are serially multiplied from \(\begin{bmatrix}[c|c] \gls*{R}_0&\gls*{T}_0 \end{bmatrix}\) until \(\begin{bmatrix}[c|c] \gls*{R}_n&\gls*{T}_n \end{bmatrix}\) % \([R_n|T_n] \)
% , this final matrix is inverted and applied to the back projected volume.
The recovered transformation matrices are then matrix inverted and applied to the back projection of the image to realign the rays in the volume to their respective source positions as shown in \figurename\ref{fig:flopt_algorithm}. %TODO hard sentence.
% This process is repeated for every angle the sum of these ray projection volumes is filtered using a high-pass filter; here a \gls*{Ram-Lak filter} is used.
% \footnote{Linear real (amplitude) ramp filter in Fourier space}. %: \(|v|\)}
% By producing a series of transformation matrices from adjacent acquisitions, errors compound and the reconstruction of volumes degrades with more projections, see \figurename~\ref{fig:irandons}.

\begin{figure*}
  \centering
  \begin{subfigure}[t]{\linewidth}
    \centering
    \includegraphics{./figures/flopt_algorithm_forward}
    \caption{Forward model}\label{fig:flopt_algorithm_forward}
  \end{subfigure}
  \\\vspace{\abovecaptionskip}
  % \quad
  \begin{subfigure}[t]{\linewidth}
    \centering
    \includegraphics{./figures/flopt_algorithm}
    \caption{Reconstruction method, solving the inverse problem.}\label{fig:flopt_algorithm_inverse}
  \end{subfigure}
  \caption[Simulation of OPT data incorporating rotational and translational offsets, and the proposed reconstruction algorithm]{
    % Two dimensional representation of the reconstruction algorithm.
    The simulation of OPT data incorporating rotational and translational offsets, and the proposed reconstruction algorithm.
    (\subref{fig:flopt_algorithm_forward}): The \(n\) projections of the object (\(\Sigma \)), at rotation (\(\gls*{R}_1\) to \(\gls*{R}_n\)) and translation (\(\gls*{T}_1\) to \(\gls*{T}_n\)), produces \(n\) frames of image data.
    During the OPT measurement, \(n\) projections of the object \(\Sigma \) are observed with rotations \(\gls*{R}_1\) to \(\gls*{R}_n\) and corresponding translations \(\gls*{T}_1\) to \(\gls*{T}_n\) where the translations account for imperfect alignment.
    (\subref{fig:flopt_algorithm_inverse}): In the reconstruction algorithm, the rotational and translational matrices are recovered (\(\gls*{R}_1'\) to \(\gls*{R}_n'\) and \(\gls*{T}_1'\) to \(\gls*{T}_n'\)) from triangulation of the fiducial markers.
    These transformation matrices are then used to obtain a contribution to the volumetric reconstruction from each observed frame and the summated reconstruction is assembled from the \(n\) frames.
    The now realigned back projections are summed to produce an unfiltered back projection.
    % and inversely applied
    % to align back project the datasets,
    The transformation matrices are shown in augmented form using homogenous coordinates.
  }\label{fig:flopt_algorithm} %TODO tidy
\end{figure*}

% The second approach is robust against compound errors but an additional programatic step is needed to know which beads in the first image correspond to beads in the \(n^{\text{th}}\) image.
% This can be is achieved using tracking and momentum particle tracking algorithms, though confounding issues can arise i.e.~if a particle orbits too far away from the imaging plane or occlusions occur.

In both cases, a decomposed \gls*{F} matrix will produce four possible transformation pairs (\gls*{R},\gls*{T}; \gls*{R},-\gls*{T}; -\gls*{R},\gls*{T}; -\gls*{R},-\gls*{T}).
Once the transformation matrix between the current view (\(n\)) and the first view is calculated, the proceeding transformation matrices are then easily chosen by similarity to the previously collected matrix and general direction of motion.
An example of this type of selection would be:
\begin{align}
  \min_{I(n)}\left[I(n) = \left(\begin{bmatrix}[c|c] \gls*{R}_n&\gls*{T}_n \end{bmatrix} - \begin{bmatrix}[c|c] \gls*{R}_{n-1}&\gls*{T}_{n-1} \end{bmatrix}\right)^2\right]
\end{align}
To find the correct matrix between the \(n=0\) and \(n=1\) orientations, each of the four matrices are compared to an ideal matrix which is composed using \emph{a priori} knowledge of the likely angle of rotation of the system's imaging properties.

\section*{Verification of the proposed algorithm}

To verify the validity and quality of the proposed reconstruction algorithm, the image of Zelda, superposed with an orthogonal image of Cameraman, is used as a testcard volume.
Virtual fiducial beads are dispersed in the volume to track the rotation and translation of the image.% and shown in \figurename~\ref{fig:raw_input}.
The reference image is then rotated through \SI{128} angles over \(2\pi \) radians and projected along the \(Y\) axis, then an image slice in (\(X,Y\)) is taken to create a single line projection, shown three dimensionally in \figurename~\ref{fig:recon_iterative}.
% using the image volume shown in \figurename~\ref{fig:ortho_3d_correct}.
This is repeated for each angle,  with each line projection stacked to create a sinogram.%, see \figurename~\ref{fig:sinogram_stretch}.

% \begin{figure*}
%   \centering
%   \hfill
%   \begin{subfigure}[t]{0.3\linewidth}
%     \includegraphics[width=\linewidth]{./figures/results/no_helix/rawinput_colour}
%     \caption{Raw input for OPT simulations, Zelda.}
%     \label{fig:raw_input}
%   \end{subfigure}\hfill
%   \begin{subfigure}[t]{0.3\linewidth}
%     \includegraphics[width=\linewidth]{./figures/results/no_helix/sinogram_stretch}
%     \caption{Image of Zelda (\figurename~\ref{fig:raw_input}) after rotation and projection in 2D, giving the sinogram.}
%     \label{fig:sinogram_stretch}
%   \end{subfigure}
%   \hfill
%   \label{fig:rawinputs}
%   \caption{Reference images for OPT reconstruction.}
% \end{figure*}

% \begin{figure}
%   % \begin{subfigure}[t]{\linewidth}
%   \centering
%   \includegraphics[width=0.4\linewidth]{./figures/ortho_3d_correct}
%   \caption{Ground truth 3D object for reconstruction, based on the Cameraman and Zelda testcard images.}\label{fig:ortho_3d_correct}
%   % \end{subfigure}
% \end{figure}
\begin{figure*}
  % \ContinuedFloat{}
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/0/xy}\caption{Raw\\Frame (\(n\)): 0\\View: (\(X,Y\))}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/0/xy_recon}\caption{Reconstructed\\Frame (\(n\)): 0\\View: (\(X,Y\))}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/0/zx}\caption{Raw\\Frame (\(n\)): 0\\View: (\(Z,Y\))}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/0/zx_recon}\caption{Reconstructed\\Frame (\(n\)): 0\\View: (\(Z,Y\))}
  \end{subfigure}
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/1/xy}\caption{Raw\\Frame (\(n\)): 1\\View: (\(X,Y\))}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/1/xy_recon}\caption{Reconstructed\\Frame (\(n\)): 1\\View: (\(X,Y\))}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/1/zx}\caption{Raw\\Frame (\(n\)): 1\\View: (\(Z,Y\))}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/1/zx_recon}\caption{Reconstructed\\Frame (\(n\)): 1\\View: (\(Z,Y\))}
  \end{subfigure}
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/dots/xy}%\caption{Raw data. Frame (\(n\)): , View: \(xy\)}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/dots/xy_recon}%\caption{Reconstructed\\Frame (\(n\)):0, View:\(xy\)}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/dots/zx}%\caption{Raw data. Frame (\(n\)): , View: \(zy\)}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/dots/zx}%\caption{Reconstructed\\Frame (\(n\)):, View:\(zx\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/31/xy}\caption{Raw\\Frame (\(n\)): 31\\View: (\(X,Y\))}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/31/xy_recon}\caption{Reconstructed\\Frame (\(n\)): 31\\View: (\(X,Y\))}\label{fig:iradon_nofilter}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/31/zx}\caption{Raw\\Frame (\(n\)): 31\\View: (\(Z,Y\))}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.2\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/3D_python/no_drift_zelda/31/zx_recon}\caption{Reconstructed\\Frame (\(n\)): 31\\View: (\(Z,Y\))}
  \end{subfigure}
  \caption[A 3D test-volume of two orthogonal and different testcard images, was used to verify the reconstructive capabilities of the proposed algorithm]{
    A 3D test-volume of two orthogonal and different testcard images, was used to verify the reconstructive capabilities of the proposed algorithm.
    The projected image data (b),~(f),~(j) and (d),~(h),~(l), are projected from (a),~(e),~(i) and (c),~(g),~(k) respectively and were used to iteratively generate reconstructions where the \(n^\text{th}\) reconstruction incorporates all the information from observation 0 to \(n\).
    The results are unfiltered for clarity of demonstrating the iterative reconstruction, which is applied in \figurename~\ref{fig:flopt_filter}.
  }\label{fig:recon_iterative}
\end{figure*}

In the standard approach for OPT reconstruction, the sinogram undergoes the inverse Radon transform, as shown in \figurename~\ref{fig:iradon_nofilter}, followed by post-filtering.
This step is substituted for the proposed algorithm; in \figurename~\ref{fig:flopt_comparison_line_profile} the two techniques are compared for ideal conditions of smooth, predictable rotation.
The proposed algorithm produces %(see )
a faithful reconstruction on the original image, as shown in \figurename~\ref{fig:filtered_recon_helix}. %with some minor deviations.
% there is good overlap between the two,
\figurename~\ref{fig:flopt_histogram} illustrates the strong overlap of the images produced by the new algorithm and the Radon transform when considering the histogram of the absolute pixel-wise difference between the original source image and the respective reconstructions.
The proposed algorithm generates lower deviance from the source image than the Radon transform.
The mean square errors (MSE, see Equation~\eqref{eq:mse}) of the new algorithm and the Radon transform are \SI{15.01}{\percent} and \SI{14.84}{\percent}, respectively, see \figurename~\ref{fig:flopt_histogram} for a histogram of a pixel-wise comparison.
%, see \figurename~\ref{fig:flopt_histogram}

\begin{align}
  \operatorname{MSE}=\frac{1}{n}\sum_{i=1}^n{(Y_i-\hat{Y_i})}^2 \label{eq:mse}
\end{align}
Where \(\mathbf{Y}\) is the vector of observed values and \(\hat{Y_i}\) is mean of the ith value of the predicted values
\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/comparison_line_profile}
    % \caption[Line profile comparison of the standard and new algorithms]{Line profile comparison of the reconstruction of a reference image computationally rotated, projected and reconstructed using the standard Radon transform and the new proposed algorithm.}
    \caption{}\label{fig:flopt_comparison_line_profile}
  \end{subfigure}\quad
  \begin{subfigure}[t]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/flopt_histogram}
    % \caption[Histogram of pixel values compared between reconstructions using flOPT and the Radon transform]{Histogram of of pixel values compared between reconstructions using flOPT and the Radon transform.
    % The shift of the histogram to towards overall lower deviance from the source image suggests the flOPT algorithm out performs the Radon transform}\label{fig:flopt_histogram}
    \caption{}\label{fig:flopt_histogram}
  \end{subfigure}\\
  \begin{subfigure}[t]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/correlation_helicity}
    % \caption[Comparison of standard and proposed OPT reconstruction algorithms for acquisitions with drift]{%2D correlation of the source image shows that flOPT does not degrade under systematic drift compared to s.
    % Comparison of standard and proposed OPT reconstruction algorithms for acquisitions with drift.
    % 2D image correlation of the ground truth and the reconstruction shows that the proposed flOPT algorithm does not degrade with systematic drift, whereas reconstruction using the standard Radon transform is severely degraded.
    % }
    \caption{}\label{fig:helical_comparison}
  \end{subfigure}
  \caption{
    (\subref{fig:flopt_comparison_line_profile}), Line profile comparison of the reconstruction of a reference image computationally rotated, projected and reconstructed using the standard Radon transform and the new proposed algorithm.
    (\subref{fig:flopt_histogram}), Histogram of pixel values compared between reconstructions using the new proposed flOPT algorithm and the Radon transform.
    The shift of the histogram towards overall lower deviance from the source image suggests the flOPT algorithm outperforms the Radon transform
    (\subref{fig:helical_comparison}): Comparison of standard and proposed OPT reconstruction algorithms for acquisitions with drift.
    2D image correlation of the ground truth and the reconstruction shows that the proposed flOPT algorithm does not degrade with systematic drift, whereas a reconstruction using the standard Radon transform is severely degraded.}
\end{figure*}

%However, the proposed algorithm fairs worse in terms of contrast compared to a Radon transform.

The more challenging case of a sample drifting systematically along the \(X\) axis, with a constant velocity, was then considered. This drift produced a helical path of a single fiducial within the sample, see \figurename~\ref{fig:flopt_helix_sinogram}.
In \figurename~\ref{fig:unfilttered_reconstruction_helix_iradon}, the Radon transform fails to produce a recognisable reproduction of the test image with the addition of a slight helicity to the rotation.
The proposed algorithm produces an equivalent result to that of a sample rotating without any systematic drift, see \figurename~\ref{fig:iradon_filter}.
In \figurename~\ref{fig:helical_comparison} the respective reconstructions from each algorithm were compared, as before, while the helical shift was incremented.
See \figurename~\ref{fig:flopt_helix_sinogram} for a sinogram of a sample wherein a helical shift has been induced.
When using correlation as a metric of reproduction quality, the new algorithm fares slightly worse at zero helicity, with \SI{94}{\percent} correlation compared to the Radon transform at \SI{96}{\percent}.
As expected, the Radon transform rapidly deteriorates once a systematic drift is applied, whereas the new algorithm maintains the quality of the reconstruction, see \figurename~\ref{fig:helical_comparison}.

\begin{figure*}
  \centering
  \hspace*{\fill}
  \begin{subfigure}[t]{0.3\linewidth}
    \begin{tikzpicture}[node distance=0cm]
      \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/helix_zelda/topdown_bead_paths}};
      \node[below=of img] {\(X\)};
      \node[left=of img,rotate=90,yshift=0.2cm] {\(Y\)};
    \end{tikzpicture}
    \caption{Top down views (\(X,Y\)) of the source image with the fiducial paths marked.}\label{fig:topdown_bead_paths}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.3\linewidth}
    \begin{tikzpicture}[node distance=0cm]
      \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/helix_zelda/sinugram_stretch}};
      \node[below=of img] {\(v\)\vphantom{\(X\)}};
      \node[left=of img,rotate=90,yshift=0.2cm] {\(n\)};
    \end{tikzpicture}
    \caption{Sinogram (\(v,n\)) of a sample whose axis of rotation has a systematic drift}\label{fig:flopt_helix_sinogram}
  \end{subfigure}\hspace*{\fill}
  \\\vspace{\abovecaptionskip}
  \hspace*{\fill}
  \begin{subfigure}[t]{0.3\linewidth}
    \begin{tikzpicture}[node distance=0cm]
      \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/helix_zelda/unfilttered_reconstruction_helix_iradon}};
      \node[below=of img] {\(X\)};
      \node[left=of img,rotate=90,yshift=0.2cm] {\(Y\)};
    \end{tikzpicture}
    \caption{Unfiltered reconstruction using a Radon transform.}\label{fig:unfilttered_reconstruction_helix_iradon}\label{fig:iradon_filter}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.3\linewidth}
    \begin{tikzpicture}[node distance=0cm]
      \node (img1) {\includegraphics[width=0.8\linewidth]{./figures/results/helix_zelda/filtered_recon_helix}};
      \node[below=of img1] {\(X\)\vphantom{\(X\)}};
      \node[left=of img1,rotate=90,yshift=0.2cm] {\(Y\)};
    \end{tikzpicture}
    \caption{Filtered reconstruction using the new algorithm.}\label{fig:filtered_recon_helix}\label{fig:flopt_filter}
  \end{subfigure}
  \hspace*{\fill}
  % \caption{}
  \caption{Comparison of the two reconstructions under sample imaging with a systematic drift, in 3D though represented here in 2D. (\subref{fig:topdown_bead_paths}) shows the path of four fiducial markers under helical drift; (\subref{fig:flopt_helix_sinogram}) shows the sinogram of this motion; with (\subref{fig:unfilttered_reconstruction_helix_iradon}) showing the result of the Radon; transform on tomographic dataset that contains this corruption whilst (\subref{fig:flopt_helix_sinogram}) shows the result of the reconstruction using the flOPT algorithm~\cite{craig_russell_2020_4073437}.}\label{fig:flopts}
\end{figure*}

\subsection*{Recovery of R and T using matrix decomposition}
To quantitatively verify that the matrix decomposition technique was valid and robust, the accuracy of the reproduction of \gls*{R} and \gls*{T} was tested directly.
The original \gls*{R} and \gls*{T} matrices were computed and compared to \gls*{R} and \gls*{T} generated from matrix decomposition. This absolute difference was computed element-wise in each matrix and then an average for each matrix was taken.
Overall, the worst-case scenario produced a percentage error of \SI{2}{\percent} (see \figurename~\ref{fig:pc_sum_decompose} for full statistics).
The accuracy of the calculated \gls*{R} and \gls*{T} deteriorated when adding in additional degrees of combined movement, but with no correlation between the degree of helicity and the error produced.
% but the severity of this movement appeared to no trending effect.
The translation matrix (\gls*{T}) was consistently more accurately reproduced, which is likely due to it having fewer available degrees of freedom.
% Consistently the translational matrix (\gls*{T}) was more accurately reproduced, this is likely due to there being fewer of degrees of freedom for errors to spread over.

%The images produced are a more faithful reproduction of the source image as the degree of helicity is increased.
%This effect may be due to the additional sampling induced by adding another degree of movement, that is the systematic drift.

%linewidth is \the\linewidth


\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.5\linewidth}
    \captionsetup{width=0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/helix/decompose/improved/10082020_bead_simulation_alpha_ROT_err}
    \caption{Rotation matrix, with angular drift in \(\alpha \)}\label{fig:pc_sum_rot_alpha}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.5\linewidth}
    \captionsetup{width=0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/helix/decompose/improved/10082020_bead_simulation_alpha_TRANS_err}
    \caption{Translation matrix, with angular drift in \(\alpha \)}\label{fig:pc_sum_trans_alpha}
  \end{subfigure}
  \par\bigskip
  \begin{subfigure}[t]{0.5\linewidth}
    \captionsetup{width=0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/helix/decompose/improved/10082020_bead_simulation_tx_ROT_err}
    \caption{Rotation matrix, with helical drift in \(x\) only}\label{fig:pc_sum_rot_tx}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.5\linewidth}
    \captionsetup{width=0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{./figures/results/helix/decompose/improved/10082020_bead_simulation_tx_TRANS_err}
    \caption{Translation matrix, with helical drift in \(x\) only}\label{fig:pc_sum_trans_tx}
  \end{subfigure}
  \bigskip
  % \begin{subfigure}[t]{0.5\linewidth}
  %   \captionsetup{width=0.8\linewidth}
  %   \centering
  %   \includegraphics[width=\linewidth]{./figures/results/helix/decompose/pc_sum_rot_both}
  %   \caption{Rotation matrix, with angular drift in \(\alpha \) and helical drift in \(x\)}\label{fig:pc_sum_rot_both}
  % \end{subfigure}\hfill
  % \begin{subfigure}[t]{0.5\linewidth}
  %   \captionsetup{width=0.8\linewidth}
  %   \centering
  %   \includegraphics[width=\linewidth]{./figures/results/helix/decompose/pc_sum_trans_both}
  %   \caption{Translation matrix, with angular drift in \(\alpha \) and helical drift in \(X\)}\label{fig:pc_sum_trans_both}
  % \end{subfigure}
  \caption[Box plots demonstrating that the rotational and translations matrices can be recovered accurately from fiducial marker positions]{Box plots demonstrating that the rotational and translations matrices can be recovered accurately from fiducial marker positions.
    Panels~(\subref{fig:pc_sum_rot_alpha}) and~(\subref{fig:pc_sum_trans_alpha}) introduce an angular drift during rotation, to an observer at the detector this would appear as a tip of the sample towards them, causing precession.
    Panels~(\subref{fig:pc_sum_rot_tx}) and~(\subref{fig:pc_sum_trans_tx}) introduce a lateral drift in \(X\) causing a helical path to be drawn out.
    % Panels~(\subref{fig:pc_sum_rot_both}) and~(\subref{fig:pc_sum_trans_both}) combine the two effects.
    In all cases, the percentage error introduced by the the addition of undesirable additional movements was on the order of \SI{<2}{\percent}.\footnote{Note that errors in recovering translation are much larger given a smaller helical shift as the percentage error of the recovery of the translation matrix is broadly constant.}
  }\label{fig:pc_sum_decompose}
\end{figure*}

\section*{Discussion}

A new algorithm for reconstructing OPT data has been demonstrated.
The new algorithm uses multiple fiducial markers to recover the matrix which describes the rotation and translation of the sample.
The quality of the reconstructions shows a slight improvement when compared to the standard Radon transform, with a great effect when a systematic drift is introduced.
The accuracy of the decomposition of \gls*{F} into \gls*{R} and \gls*{T} was compared to the ground truth matrices.
The element-wise absolute difference \(\left(\frac{x-y}{2(x+y)}\right)\) of each matrix was averaged across the matrix for \gls*{R} and \gls*{T}.
In the worst-case scenario, a maximum of \SI{2}{\percent} average absolute difference was found between ground truth and recovered matrices,
% When comparing the
% expected matrices to the recovered matrices a
% ground truth matrices to the recovered matrices were compared using the average of the element-wise using square differences
% peak of \SI{2}{\percent} difference is found between the two when considering worst case scenarios;
suggesting that the technique is robust to various forms of drift in all dimensions and general instability.
Such an algorithm could be used to minimise ghosting effects seen in real samples, particularly in samples where slipping is likely to occur, such as in gels or in cheaper OPT systems which tend to be more mechanically unstable and imprecise.
In particular the imaging of large mobile gels is set to become more prevalent given the surge of new techniques in Expansion Microscopy~\cite{chenExpansionMicroscopy2015}, whereby fragile expanded samples embedded in thin lubricious gels.

\section*{Future work}

The proposed algorithm relies on triangulation between two view points.
% In this work the two view points refer to the image at frames \(n\) and \(n+1\).
However, it is possible to use three separate views %(frames \(n\), \(n+1\) and \(n+2\))
to reconstruct a scene, one such approach being quaternion tensors~\cite{hartleyMultipleViewGeometry}.
Working with tensors is more complex, but a future iteration of the algorithm presented here may benefit from using three views to provide a more accurate transformation matrix.
Beyond three views, there is currently no mathematical framework for four or more views.
If such tools were to be developed, it may be possible to have the algorithm described above be a non-iterative, single-shot reconstruction from pixels to voxels.

% In computer vision, scenes often do not contain known fiducial marks and so such marks are found between views.
Fiducial markers could also be extracted from the image texture alone, circumventing the need for the additional beads embedded in the sample.
To find such correspondences, points with similar local texture are found and matched in between each image using standard algorithms such as SIFT~\cite{loweObjectRecognitionLocal1999} and RANSAC~\cite{fischlerRandomSampleConsensus1981}. %, many of these such correspondences are found
This was attempted in this work, however, the errors introduced into the transformation matrices make this approach currently unviable; and so by requiring bright punctuate fiducial markers the burden of collecting the fiducial coordinates is shifted to well established curve fitting algorithms that are robust to noise.
% This technique is only valid for views with small angles between them, as would be found in OPT.

% \begin{acknowledgements}
% % \blindtext
% \end{acknowledgements}

\section*{Code availability}
All of the code presented here is FOSS using OpenCV and Python for our simulation, and can be found on GitHub~\cite{craig_russell_2020_4073437}.

\section*{Disclosures}
The authors declare that there are no conflicts of interest related to this article.

\onecolumn
\newpage
%
% \section*{Word Counts}
% This section is \textit{not} included in the word count.
% \subsection*{Notes on Nature Methods Brief Communication}
% \begin{itemize}
% \item Abstract: 3 sentences, 70 words.
% \item Main text: 3 pages, 2 figures, 1000-1500 words, more figures possible if under 3 pages
% \end{itemize}

% \subsection*{Statistics on word count}
% \detailtexcount
% \newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Supplementary Information %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak


% \ifdefined\osa
% \fi
\ifdefined\bioarxiv
  \captionsetup*{format=largeformat}
\fi

%
% \pagebreak
\section{Reconstruction}
% \subsection{}

As the sample is rotated each detector pixel collects an intensity \(I(\theta) = I_{n}e^{-k(\theta)}\) at discrete (\(n\)) angles through a full rotation of the sample; where \(I_{n}\) is the unattenuated radiation intensity from the source to the detector, \(k\) is the attenuation caused by the sample along a detected ray \(I(n)\) is the measured intensity, see \figurename~\ref{fig:OPT_digram}.
Rays from the sample to the detector approximate straight lines, and so the rays reaching the detector can be represented with line integrals.
A projection is then the resulting intensity profile at the detector for a rotation angle, and the integral transform that results in \(P_\theta(v)\)
% \(f(I_i,\theta_n) \)
is the Radon transform.
% This is defined mathematically as:

\begin{align}
  \intertext{The equation of a set of parallel rays from a source passing through the specimen to a point \(v\) along the detector is:}
                & X\cos(\theta) + Y\sin(\theta) - v = 0                                                            \\
  \intertext{Projecting many such rays through a sample with structure \(f(X,Y)\) gives:}
  P_\theta(v) = & \int_{\infty}^{\infty} \int_{\infty}^{\infty} f(X,Y)\delta (x\cos(\theta)+y \sin(\theta)-v)dX dY
\end{align}

% \begin{figure}
%     \centering
%     \includegraphics{coordinate_system}
%     \caption{Coordinate system}
%     \label{fig:coordinate_system_flopt}
% \end{figure}

Where \(P_\theta(v)\) is the Radon transform of \(f(X,Y)\) which represents the contrast image of a 2D slice of the specimen.
The Radon transform of an image produces a sinogram.% as in \figurename~\ref{fig:rawinputs}


% A parallel projection is then just the combination of line integrals  \(f(I) \) for a constant. % for a constant.

An inverse Radon transform is used to recover the original object from the projection data; which is achieved by taking the Fourier transform of each projection measurement, then reordering the information from the sample into the respective position in Fourier space.
This is valid due to the Fourier Slice theorem~\cite{bracewellStripIntegrationRadio1956}
, which states that the Fourier transform of a parallel projection is equivalent to a 2D slice of the Fourier transform of the original sample.
The back projection step is given by:
%A high pass filter such as a ramp filter is commonly used to counter the blurring caused by this oversampling.
% FBP can be thought of as smearing the projection data across the image plane, and is expressed in equation form as:
\begin{align}
  f_{\text{fpb}}(X,Y) = \int_{0}^{\pi} Q_\theta (X\cos(\theta)+Y\sin(\theta),\theta)dXdY
\end{align}

Where \(Q_\theta \) is the filtered projection data, and \(f_{\text{fpb}}(X,Y)\) is the back-projected image~\cite{dudgeonMultidimensionalDigitalSignal1984}.
A spatial filtering step is applied during back-projection  to avoid spatial frequency oversampling during the object’s rotation
%  (see \figurename~\ref{fig:iradon_filter})
a high pass filter is commonly used to compensate for the perceived blurring.
The blurring arises as \(Q_\theta \) is back-projected (smeared) across the image plane for each angle of reconstruction; which means that not only does the back-projection contribute at the line it is intended to (along line \(C\) in \figurename~\ref{fig:OPT_digram}), but all other points along the back-projecting ray.
% The blurring occurs as \(Q_\theta\) makes the same contribution to the reconstruction for each angle
% will make the same contribution to the reconstruction at all of these points. Therefore, one could say that in the reconstruction process each filtered projection, Qe, is smeared back, or backprojected, over the image plane.


Now, suppose we know the relative positions of the two cameras and their respective intrinsic parameters, such as magnification and pixel offset.
For a single camera and given the camera parameters, we can translate pixel coordinates, \(\gls*{w} = (u, v)\), into the coplanar image plane coordinates \(\gls*{x}=(x, y)\):

\begin{align}
  u & = u_0 + k_u x \\
  v & = v_0 + k_v y
\end{align}

Knowing the focal length (\(f\)) of the imaging system, image plane coordinates may be projected into a ray in 3D.
The ray can be defined by using the point \gls*{p} in camera-centred coordinates, where it crosses the image plane.

\begin{align}
  \mathbf{p} = \begin{bmatrix}
    x \\y
  \end{bmatrix}
\end{align}

From the definition of a world point, as observed through an image, we can construct a dual-view model of world points in space as in \figurename~\ref{fig:epi-polar-geom}.
Using a model of a system with two views allows for the triangulation of rays based on image correspondences; this is an important part of stereo-vision.
The most important matching constraint which can be used is the \emph{epipolar constraint}, and follows directly from the fact that the rays must intersect in 3D space.
Epipolar constraints facilitate the search for correspondences, they constrain the search to a 1D line in each image.
To derive general epipolar constraints, one should consider the epipolar geometry of two cameras as seen in \figurename~\ref{fig:epi-polar-geom}


The \textbf{baseline} is defined as the line joining the optical centres.
An \textbf{epipole} is the point of intersection of the baseline with the image plane and there are two epipoles per feature, one for each camera.
An \textbf{epipolar line} is a line of intersection of the epipolar plane with an image plane.
It is the image in one camera of the ray from the other camera’s optical centre to the world point (\gls*{X}).
For different world points, the epipolar plane rotates about the baseline.
All epipolar lines intersect the epipole.

The epipolar line constrains the search for correspondence from a region to a line.
If a point feature is observed at \gls*{x} in one image frame, then its location \gls*{x'} in the other image frame must lie on the epipolar line.
We can derive an expression for the epipolar line.
The two camera-centered coordinate systems \gls*{X_c'} and \gls*{X_c} are related by a rotation, \gls*{R} and translation, \gls*{T} (see in \figurename~\ref{fig:epi-polar-geom}) as follows:

\begin{align}
  \gls*{X_c'} & = \gls*{R}\gls*{X_c'} + \gls*{T} \label{eq:Xprime = RTX}
\end{align}
%       \\
%     \intertext{Taking the vector product with \(\gls*{T}\), we obtain}
%     \gls*{T} \times \gls*{X_c'} &= \gls*{T} \times \gls*{R}\gls*{X_c}+ \gls*{T} \times \gls*{T}  \\
%     \gls*{T} \times \gls*{X_c'} &= \gls*{T} \times \gls*{R}\gls*{X_c}\label{eq:Xprime = RTX}
% \end{align}

% \begin{figure}
%   \centering
%   \includegraphics{./figures/epi-polar-geom}
%   \caption[Epi-polar geometry described for two adjacent views]{
%   Epi-polar geometry described for two adjacent views (or cameras of a scene).
%   Coordinates as expressed in \figurename~\ref{fig:coordinate_system_flopt} with prime notation (\('\)) denoting the additional right camera view.
%   Transforming from right to left camera-centered coordinates (\gls*{X_c'} to \gls*{X_c}) requires a rotation (\gls*{R}) and a translation (\gls*{T}).
%   }\label{fig:epi-polar-geom}
% \end{figure}

\subsection{The Essential matrix}

\begin{align}
  \intertext{Taking the scalar product of~\eqref{eq:Xprime = RTX} with \(\gls*{X_c'}\), we obtain:}
  \gls*{X_c'} \cdot (\gls*{T} \times \gls*{X_c})          & = \gls*{X_c'}\cdot (\gls*{T} \times \gls*{R} \gls*{X_c'}) \\
  \gls*{X_c'} \cdot (\gls*{T} \times \gls*{R} \gls*{X_c}) & = 0
  \intertext{A vector product can be expressed as a matrix multiplication:}
  \gls*{T} \times \gls*{X_c}                              & = \gls*{T_x} \gls*{X_c}
  \intertext{where}
  \gls*{T_x}                                              & =\begin{bmatrix}
    0    & -T_z & T_y  \\
    T_z  & 0    & -T_x \\
    -T_y & T_x  & 0
  \end{bmatrix}
\end{align}
So equation~\eqref{eq:Xprime = RTX} can be rewritten as:

\begin{align}
  \gls*{X_c'} \cdot (\gls*{T_x} \gls*{R}\gls*{X_c}) = 0 \\
  \gls*{X_c'} \gls*{T} \gls*{Essential} \gls*{X_c}= 0   \\
  \intertext{where}
  \gls*{Essential} = \gls*{T_x} \gls*{R}
\end{align}

\gls*{Essential} is a \(3 \times 3\) matrix known as the \emph{essential matrix}.
The constraint also holds for rays \gls*{p}, which are parallel to the camera-centered position vectors \gls*{X_c}:

\begin{align}
  \gls*{p'}^T \gls*{Essential} \gls*{p} = 0 \label{eq:pEp}
\end{align}
This is the epipolar constraint.
If a point \gls*{p} is observed in one image, then its position \gls*{p'} in the other image must lie on the line defined by Equation~\eqref{eq:pEp}.
The essential matrix can convert from pixels on the detector to rays \gls*{p} in the world, assuming a calibrated camera (intrinsic properties are known), and pixel coordinates can then be converted to image plane coordinates using:
\begin{align}
  \begin{bmatrix}
    u \\
    v \\
    1
  \end{bmatrix}
                       & =
  \begin{bmatrix}
    k_u & 0   & u_0 \\
    0   & k_v & v_0 \\
    0   & 0   & 1
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y \\
    1
  \end{bmatrix}
  \intertext{We can modify this to derive a relationship between pixel coordinates and rays:}
  \begin{bmatrix}
    u \\
    v \\
    1
  \end{bmatrix}
                       & =
  \begin{bmatrix}
    \frac{k_u}{f} & 0             & \frac{u_0}{f} \\
    0             & \frac{k_v}{f} & \frac{v_0}{f} \\
    0             & 0             & \frac{1}{f}
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y \\
    f
  \end{bmatrix}
  \intertext{\(\widetilde{\gls*{K}}\) is defined as follows:}
  \widetilde{\gls*{K}} & = \begin{bmatrix}
    f k_u & 0     & u_0 \\
    0     & f k_v & v_0 \\
    0     & 0     & 1
  \end{bmatrix}
  \intertext{then we can write pixel coordinates in homogenous coordinates:}
  \widetilde{\gls*{w}} & = \widetilde{\gls*{K}} \gls*{p}
\end{align}

\subsection{The Fundamental matrix}

\begin{align}
  \intertext{From~\eqref{eq:pEp} the epipolar constraint becomes}
  % \mathbf{p'}^T E \mathbf{p} &= 0 \\
  \widetilde{\gls*{w'}} ^T \widetilde{\gls*{K}}^{-T} \gls{Essential} \widetilde{\gls*{K}}^{-1} \widetilde{\gls*{w}} & = 0 \\
  \widetilde{\gls*{w'}} ^T F \widetilde{\gls*{w}}                                                                   & = 0
\end{align}
%\subsubsection{Two views}
%\paragraph{Mapping from one camera to another}
%\subsubsection{Three and more views}
The (\(3\times3)\) matrix \gls*{F}, is the called the \emph{fundamental matrix}.
With intrinsically calibrated cameras, structure can be recovered by triangulation.
First, the two projection matrices are obtained via a SVD of the essential matrix,
the SVD of the essential matrix is given by:

\begin{align}
  \gls*{Essential} & = \gls*{K'}^T \gls*{F} \gls*{K} = \gls*{T_x} \gls*{R} = \mathbf{U}\Lambda \mathbf{V}^T \\%\gls*{R} = U\LambdaV^T\\
  \intertext{It can be shown that}
  \hat{\gls*{T_x}} & = U \begin{bmatrix}
    0  & 1 & 0 \\
    -1 & 0 & 0 \\
    0  & 0 & 0
  \end{bmatrix} U^T
  \intertext{ and }
  \gls*{R}         & = U \begin{bmatrix}
    0 & -1 & 0 \\
    1 & 0  & 0 \\
    0 & 0  & 1
  \end{bmatrix} V^T
  \intertext{Then, aligning the left camera and world coordinate systems gives the projection matrices:}
  \mathbf{P}       & = \gls*{K}    \begin{bmatrix}[c|c]       \mathbf{I} & \mathbf{0}   \end{bmatrix}
  \intertext{ and }
  \mathbf{P}'      & = \gls*{K'} \begin{bmatrix}[c|c]       \gls*{R} & \gls*{T}   \end{bmatrix}
\end{align}
Where \( \begin{bmatrix}[c|c] \mathbf{I} & \mathbf{0} \end{bmatrix}\) is the identity matrix augmented column-wise with a zero matrix, and the two projection matrices (\(\mathbf{P}\) and \(\mathbf{P}'\)) project from camera pixel coordinates to world coordinates.
Given these projection matrices, scene structure can be recovered (only up to scale, since only the magnitude of \gls*{T} (|\gls*{T}|) is unknown) using least squares fitting.
Ambiguities in \gls*{T} and \gls*{R} are resolved by ensuring that visible points lie in front of the two cameras.
As with the essential matrix, the fundamental matrix can be factorised into a skew-symmetric matrix corresponding
to translation and a \(3 \times 3\) non-singular matrix corresponding to rotation.



% \bibliography{filler,thesis,xtrareferences}
% \bibliographystyle{zHenriquesLab-StyleBib}
% \bibliography{thesis,filler,xtrareferences,zenodo,2019_flopt}
% \bibliography{thesis,filler,xtrareferences,zenodo,2019_flopt,2020_flopt,2020}
%% You can use these special %TC: tags to ignore certain parts of the text.
%TC:ignore
%the command above ignores this section for word count

%
% \begin{figure}
%   \centering
%    \hspace*{\fill}
%   \begin{subfigure}[t]{0.3\linewidth}
%     \includegraphics[width=\linewidth]{./figures/results/no_helix/iradon_nofilter}
%     \caption[Unfiltered iRadon]{This is the unfiltered reconstruction of the object using the \gls*{Radon transform}}%TODO equation???
%     \label{fig:iradon_nofilter}
%   \end{subfigure}\hfill
%   \begin{subfigure}[t]{0.3\linewidth}
%     \includegraphics[width=\linewidth]{./figures/results/no_helix/iradon_filter}
%     \caption[Filtered iRadon]{Ram-lak (Fourier ramp) filter applied to \figurename~\ref{fig:iradon_nofilter}.}
%     \label{fig:iradon_filter}
%   \end{subfigure}
%      \hspace*{\fill}
%     % \label{fig:irandons}
%   \caption{The result of a tomographic reconstruction (using equally spaced angular steps and no translation between frames) requires Fourier filtering to normalise spatial contrast.}\label{fig:irandons}%TODO simulations with no drift and skew and use equation
% \end{figure}

The second approach is less prone to compound errors but relies on precise identification and tracking of fiducial markers.
distinction and tracking fiducials.
Instead of calculating \gls*{F} between neighbouring images, \gls*{F} is calculated between the current projection and the very first projection.
\gls*{F} is then decomposed and the transformation matrix is inverted and applied to the back projected volume.
The reoriented back projected volumes are summed and finally filtered to remove the additional spatial frequencies imparted from rotating the sample.

% \subsection{Bead tracking}

% \section{Future work}
%
% \subsection{Bead tracking}
% The work presented here is, so far, is a proof-of-concept demonstrated by simulation and reconstruction from ground-truth testcard objects, and requires several further steps in order apply it to real \gls*{OPT} data.
% Firstly, a bead-tracking algorithm~\cite{crockerMethodsDigitalVideo1996} will need to be created to reliably track multiple beads, concurrently, in an image series.
% A sensible approach would be to have a user select the fiducial markers in the image on the first frame and template match in a small window around the selection; this is similar to the algorithm described in Chapter~\ref{chapter:spt}.
% Template matching is robust to occlusions provided the fiducial is not fully eclipsed.
% If two fiducial markers occlude each other however, this algorithm may switch their identities or both tracking windows may follow one bead.
% This is a common problem in particle tracking algorithms, but is solved by using a weighted likelihood based on momentum\cite{chenouardMultipleHypothesisTracking2013}.
%
% The likelihood of a bead occlusion occurring will increase with he introduction of additional beads into the sample.
% As such occluded beads may need to be omitted. % and possibly interpolated for.

% Egregious outliers may be found by tracking a \emph{confidence} estimator as the bead rotates as in
% A primitive estimator would be the pixel-wise sum of intensities in the result of a correlative template matching.
% Whilst this confidence value itself has no physical interpretation, any stark changes in the derivative will be suggestive of an occlusion or mis-tracking of some variety, see \figurename~\ref{fig:confidence_bead_tracking}.

% \begin{figure}
%   \centering
%   \includegraphics{./figures/results/confidence_bead_tracking}
%   \caption{A plot of the confidence value of a single fiducial bead being tracking \emph{in vivo}.
%   Sharp changes in the confidence value occur when the fiducial bead is occluded.
%   The image at the origin shows the fiducial being tracked well in the first frame.
%   (Images courtesy of Pedro Vallejo)
%   }
%   \label{fig:confidence_bead_tracking}
% \end{figure}

% \subsection{Multiple views tracking}
% The theory backing the proposed algorithm relies on triangulation between two view points.
% In this work the two view points refer to the image at frames \(n\) and \(n+1\).
% However, it is possible to use three separate views (frames \(n\), \(n+1\) and \(n+2\)) to reconstruct a scene, one such approach being quaternion tensors.
% Working with tensors is computationally and mathematically more challenging, but a future iteration of the algorithm presented here may benefit from using three views to provide a more accurate transformation matrix.
% Beyond three views there currently is no mathematical framework at present for four or more views.
% If such tools did exist, it may be possible to make the algorithm described above as non-iterative and essentially a single shot reconstruction from pixels to voxels.

% \subsection{Fiducial free reconstruction}
%
% In computer vision, scenes often do not contain known fiducial marks and so such marks are found between views.
% To find such a correspondences, points with similar local texture are found and matched in between each image. %, many of these such correspondences are found
% This technique is only valid for views with small angles between them, as would be found in \gls*{OPT}.
% A similar method could be introduced into the algorithm presented here, as each image should have sufficient texture, particularly when using \gls*{tOPT}.
%
% The following chapter will move from improvements in registration using projective matrices and into improvements in resolution using on-camera slit-scanning.

%A future version of this algorithm may be able to use 3 views to produce a more faithful transformation matrix.
%The mathematics for more than three views currently does not exist.


% \begin{figure}
%   \centering
%   \includegraphics{./figures/flOPT_principle}
%   \caption[Principles of the proposed algorithm]{Principles of the proposed algorithm. Each successive frame of OPT image data will have an associated \gls*{R} and \gls*{T} (shown here in augmented form using homogenous coordinates), these matrices can be recovered from comparing the fiducial marker positions in each frame (\(n\)) and its successor (\(n+1\)).}
% \end{figure}



% \begin{figure}
%   \centering
%   \hspace*{\fill}
%   \begin{subfigure}[t]{0.3\linewidth}
%     \begin{tikzpicture}[node distance=0cm]
%       \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/no_helix/iradon_nofilter}};
%       \node[below=of img] {\(X\)};
%       \node[left=of img,rotate=90,yshift=0.2cm] {\(Y\)};
%     \end{tikzpicture}
%     \caption[Unfiltered iRadon]{This is the unfiltered reconstruction of the object using the Radon transform}\label{fig:iradon_nofilter}%TODO equation???
%   \end{subfigure}\hfill
%   \begin{subfigure}[t]{0.3\linewidth}
%     \begin{tikzpicture}[node distance=0cm]
%       \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/no_helix/iradon_filter}};
%       \node[below=of img] {\(v\)\vphantom{\(X\)}};
%       \node[left=of img,rotate=90,yshift=0.2cm] {\(Y\)};
%     \end{tikzpicture}
%     \caption[Filtered iRadon]{Ram-Lak (Fourier ramp) filter applied to \figurename~\ref{fig:iradon_nofilter}.}\label{fig:iradon_filter}
%   \end{subfigure}
%   \hspace*{\fill}
%   \caption[Effects of filtering the result of a inverse Radon transform reconstruction]{
%     The result of a tomographic reconstruction (using equally spaced angular steps and no translation between frames) requires Fourier filtering to normalise spatial contrast.}\label{fig:irandons}%TODO simulations with no drift and skew and use equation
% \end{figure}


% \begin{figure}
%   \centering
%   \hspace*{\fill}
%   \begin{subfigure}[t]{0.3\linewidth}
%     \begin{tikzpicture}[node distance=0cm]
%       \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/no_helix/flopt_camerman}};
%       \node[below=of img] {\(Z\)};
%       \node[left=of img,rotate=90,yshift=0.2cm] {\(Y\)};
%     \end{tikzpicture}
%     \caption{Filtered reconstruction of the Cameraman testcard}
%   \end{subfigure}\hfill
%   \begin{subfigure}[t]{0.3\linewidth}
%     \begin{tikzpicture}[node distance=0cm]
%       \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/no_helix/flopt_filter}};
%       \node[below=of img] {\(X\)};
%       \node[left=of img,rotate=90,yshift=0.2cm] {\(Y\)};
%     \end{tikzpicture}
%     \caption{Filtered reconstruction of the Zelda testcard}
%   \end{subfigure}
%   \hspace*{\fill}
%   \caption[Filtered reconstruction of the ground truth reference image using the new proposed algorithm.]{
%     Filtered reconstruction of the ground truth reference image from \figurename~\ref{fig:recon_iterative} using the new proposed algorithm.
%   }\label{fig:flopt_filter}
% \end{figure}


%
% \begin{figure}
%   \centering
%    \hspace*{\fill}
%   \begin{subfigure}[t]{0.3\linewidth}
%     \begin{tikzpicture}[node distance=0cm]
%       \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/helix/topdown_bead_paths}};
%        \node[below=of img] {\(X\)};
%        \node[left=of img,rotate=90,yshift=0.2cm] {\(Y\)};
%     \end{tikzpicture}
%     \caption{Top down views (\(X,Y\)) of the source image with the fiducial paths marked.}\label{fig:topdown_bead_paths}
%   \end{subfigure}\hfill
%   \begin{subfigure}[t]{0.3\linewidth}
%     \begin{tikzpicture}[node distance=0cm]
%       \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/helix/sinugram_stretch}};
%        \node[below=of img] {\(v\)\vphantom{\(X\)}};
%        \node[left=of img,rotate=90,yshift=0.2cm] {\(n\)};
%     \end{tikzpicture}
%     \caption{Sinugram (\(v,n\)) of a sample whose axis of rotation has a systematic drift}\label{fig:flopt_helix_sinugram}
%   \end{subfigure}
%    \hspace*{\fill}
%    \caption{Comparison of the two reconstructions under sample imaging with a systematic drift, in 3D though represented here in 2D.}\label{fig:flopts}
% \end{figure}

% \begin{figure}
%   \centering
%    \hspace*{\fill}
%   \begin{subfigure}[t]{0.3\linewidth}
%     \begin{tikzpicture}[node distance=0cm]
%       \node (img) {\includegraphics[width=0.75\linewidth]{./figures/results/no_helix/rawinput_colour}};
%        \node[below=of img] {\(X\)};
%        \node[left=of img,rotate=90,yshift=0.2cm] {\(Y\)};
%     \end{tikzpicture}
%     \caption{Ground truth image for OPT simulations, Zelda (\(f(X, Y)\)).}\label{fig:raw_input}
%   \end{subfigure}\hfill
%   \begin{subfigure}[t]{0.3\linewidth}
%     \begin{tikzpicture}[node distance=0cm]
%       \node (img) {\includegraphics[width=0.8\linewidth]{./figures/results/no_helix/sinugram_stretch}};
%        \node[below=of img] {\(v\)\vphantom{\(X\)}};
%        \node[left=of img,rotate=90,yshift=0.2cm] {\(\theta\)};
%     \end{tikzpicture}
%     \caption{Image of Zelda (\figurename~\ref{fig:raw_input}) after rotation and projection in 2D, giving the sinugram (\(P_{\theta}(v)\)).
%     }\label{fig:sinugram_stretch}
%   \end{subfigure}
%    \hspace*{\fill}
%   \caption{Reference images for OPT reconstruction.}\label{fig:rawinputs}
% \end{figure}


% \immediate\write18{bash zip.sh}

% \section*{Bibliography}
% \bibliography{filler,thesis,xtrareferences}
% \bibliographystyle{zHenriquesLab-StyleBib}
% \bibliography{thesis,filler,xtrareferences,zenodo,2019_flopt}
% \bibliography{thesis,filler,xtrareferences,zenodo,2019_flopt,2020_flopt}
% \printbibliography
\section*{Bibliography}
\bibliography{merge}
% \printbibliography
\end{document}
